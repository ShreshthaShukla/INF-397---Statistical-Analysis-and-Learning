summary(glm.fit)
glm.probs <- predict(glm.fit, testset, type="response")
glm.preds <- ifelse(glm.probs>0.5, 1, 0)
confusion_matrix_glm <- table(testset$final_quality,glm.preds)
print(confusion_matrix_glm)
MyData <- read.csv(file="redwine.csv", header=TRUE, sep=",")
str(MyData)
library(psych)
describe(MyData)
summary(MyData)
pairs(MyData, main="Scatter Plot of Variables")
z <- cor(MyData)
z[lower.tri(z,diag=TRUE)]=NA
z=as.data.frame(as.table(z))
z=na.omit(z)
z=z[order(-abs(z$Freq)),]
head(z, n=5)
MyData$final_quality <- with(ifelse(quality>mean(quality), 1, 0), data=MyData)
myvars <- names(MyData) %in% c("quality")
fulldataset <- MyData[!myvars]
head(fulldataset, n=1)
set.seed(1)
rows <- sample(x=nrow(fulldataset), size=.80*nrow(fulldataset))
trainset <- fulldataset[rows, ]
testset <- fulldataset[-rows, ]
glm.fit <- glm(final_quality ~ fixed.acidity+volatile.acidity+citric.acid+residual.sugar+chlorides+free.sulfur.dioxide+total.sulfur.dioxide+density+pH+sulphates+alcohol, data=trainset, family=binomial)
summary(glm.fit)
glm.probs <- predict(glm.fit, testset, type="response")
glm.preds <- ifelse(glm.probs>0.5, 1, 0)
confusion_matrix_glm <- table(testset$final_quality,glm.preds)
print(confusion_matrix_glm)
Correct_Predictions_fraction = (confusion_matrix_glm[1,1]+confusion_matrix_glm[2,2])/sum(confusion_matrix_glm)
sprintf("Fraction of Correction Predictions are: %f",Correct_Predictions_fraction)
library(class)
set.seed(1)
set.seed(1)
variables <- which(names(fulldataset)%in%c("fixed.acidity","volatile.acidity","citric.acid","residual.sugar","chlorides","free.sulfur.dioxide","total.sulfur.dioxide","density","pH","sulphates","alcohol"))
test_error <- data.frame("k"=1:11)
for(k in 1:11)
{
knn.pred <- knn(train=trainset[, variables], test=testset[, variables], cl=testset$final_quality, k=k)
test_error$error[k]= round(sum(knn.pred!=fulldataset$final_quality)/nrow(fulldataset)*100,2)
}
for(k in 1:11)
{
knn.pred <- knn(train=trainset[, variables], test=testset[, variables], cl=trainset$final_quality, k=k)
test_error$error[k]= round(sum(knn.pred!=fulldataset$final_quality)/nrow(fulldataset)*100,2)
}
test_error
library(class)
MyData <- read.csv(file="redwine.csv", header=TRUE, sep=",")
str(MyData)
library(psych)
describe(MyData)
summary(MyData)
pairs(MyData, main="Scatter Plot of Variables")
z <- cor(MyData)
z[lower.tri(z,diag=TRUE)]=NA
z=as.data.frame(as.table(z))
z=na.omit(z)
z=z[order(-abs(z$Freq)),]
head(z, n=5)
MyData$final_quality <- with(ifelse(quality>mean(quality), 1, 0), data=MyData)
myvars <- names(MyData) %in% c("quality")
fulldataset <- MyData[!myvars]
head(fulldataset, n=1)
set.seed(1)
rows <- sample(x=nrow(fulldataset), size=.80*nrow(fulldataset))
trainset <- fulldataset[rows, ]
testset <- fulldataset[-rows, ]
glm.fit <- glm(final_quality ~ fixed.acidity+volatile.acidity+citric.acid+residual.sugar+chlorides+free.sulfur.dioxide+total.sulfur.dioxide+density+pH+sulphates+alcohol, data=trainset, family=binomial)
summary(glm.fit)
glm.probs <- predict(glm.fit, testset, type="response")
glm.preds <- ifelse(glm.probs>0.5, 1, 0)
confusion_matrix_glm <- table(testset$final_quality,glm.preds)
print(confusion_matrix_glm)
Correct_Predictions_fraction = (confusion_matrix_glm[1,1]+confusion_matrix_glm[2,2])/sum(confusion_matrix_glm)
sprintf("Fraction of Correction Predictions are: %f",Correct_Predictions_fraction)
variables <- which(names(fulldataset)%in%c("fixed.acidity","volatile.acidity","citric.acid","residual.sugar","chlorides","free.sulfur.dioxide","total.sulfur.dioxide","density","pH","sulphates","alcohol"))
test_error <- data.frame("k"=1:11)
set.seed(1)
for(k in 1:11)
{
knn.pred <- knn(train=trainset[, variables], test=testset[, variables], cl=trainset$final_quality, k=k)
test_error$error[k]= round(sum(knn.pred!=fulldataset$final_quality)/nrow(fulldataset)*100,2)
}
test_error
set.seed(1)
for(k in 1:11)
{
knn.pred <- knn(train=trainset[, variables], test=testset[, variables], cl=trainset$final_quality, k=k)
test_error$error[k]= round(sum(knn.pred!=testset$final_quality)/nrow(testset)*100,2)
}
test_error
print(test_error)
set.seed(1)
rows <- sample(x=nrow(fulldataset), size=0.8*nrow(fulldataset))
train_set <- fulldataset[rows, ]
test_set <- fulldataset[-rows, ]
set.seed(1)
rows <- sample(x=nrow(fulldataset), size=0.8*nrow(fulldataset))
trainset <- fulldataset[rows, ]
testset <- fulldataset[-rows, ]
library (MASS)
lda.fit <- lda(final_quality ~ fixed.acidity+volatile.acidity+citric.acid+residual.sugar+chlorides+free.sulfur.dioxide+total.sulfur.dioxide+density+pH+sulphates+alcohol, data=trainset)
lda.pred <- predict(lda.fit, testset)
confusion_matrix_lda <- table(testset$final_quality, lda.pred$class)
print(confusion_matrix_lda)
round(sum(lda.pred$class!=testset$final_quality)/nrow(testset),3)
qda.fit <- qda(final_quality ~ fixed.acidity+volatile.acidity+citric.acid+residual.sugar+chlorides+free.sulfur.dioxide+total.sulfur.dioxide+density+pH+sulphates+alcohol, data=trainset)
qda.pred <- predict(qda.fit, testset)
confusion_matrix_qda <- table(testset$final_quality, qda.pred$class)
print(confusion_matrix_qda)
round(sum(qda.pred$class!=testset$final_quality)/nrow(testset),3)
print("The test error is:", round(sum(lda.pred$class!=testset$final_quality)/nrow(testset),3))
print("The test error is")
round(sum(lda.pred$class!=testset$final_quality)/nrow(testset),3)
print("The test error is", test_error_lda)
test_error_lda <- round(sum(lda.pred$class!=testset$final_quality)/nrow(testset),3)
print("The test error is", test_error_lda)
print("The test error is %i", test_error_lda)
print("The test error is %s", test_error_lda)
sprintf("The test error is: %f", test_error_lda)
test_error_qda <- round(sum(qda.pred$class!=testset$final_quality)/nrow(testset),3)
sprintf("The test error is: %f", test_error_qda)
test_error_qda <- sum(qda.pred$class!=testset$final_quality)/nrow(testset)
sprintf("The test error is: %f", test_error_qda)
test_error_lda <- sum(lda.pred$class!=testset$final_quality)/nrow(testset)
sprintf("The test error is: %f", test_error_lda)
library(class)
MyData <- read.csv(file="redwine.csv", header=TRUE, sep=",")
str(MyData)
library(psych)
describe(MyData)
summary(MyData)
pairs(MyData, main="Scatter Plot of Variables")
z <- cor(MyData)
z[lower.tri(z,diag=TRUE)]=NA
z=as.data.frame(as.table(z))
z=na.omit(z)
z=z[order(-abs(z$Freq)),]
head(z, n=5)
MyData$final_quality <- with(ifelse(quality>mean(quality), 1, 0), data=MyData)
head(z, n=10)
MyData$final_quality <- with(ifelse(quality>mean(quality), 1, 0), data=MyData)
myvars <- names(MyData) %in% c("quality")
fulldataset <- MyData[!myvars]
head(fulldataset, n=1)
set.seed(1)
rows <- sample(x=nrow(fulldataset), size=.80*nrow(fulldataset))
trainset <- fulldataset[rows, ]
testset <- fulldataset[-rows, ]
glm.fit <- glm(final_quality ~ fixed.acidity+volatile.acidity+citric.acid+residual.sugar+chlorides+free.sulfur.dioxide+total.sulfur.dioxide+density+pH+sulphates+alcohol, data=trainset, family=binomial)
summary(glm.fit)
glm.probs <- predict(glm.fit, testset, type="response")
glm.preds <- ifelse(glm.probs>0.5, 1, 0)
confusion_matrix_glm <- table(testset$final_quality,glm.preds)
print(confusion_matrix_glm)
Correct_Predictions_fraction = (confusion_matrix_glm[1,1]+confusion_matrix_glm[2,2])/sum(confusion_matrix_glm)
sprintf("Fraction of Correction Predictions are: %f",Correct_Predictions_fraction)
variables <- which(names(fulldataset)%in%c("fixed.acidity","volatile.acidity","citric.acid","residual.sugar","chlorides","free.sulfur.dioxide","total.sulfur.dioxide","density","pH","sulphates","alcohol"))
test_error <- data.frame("k"=1:11)
set.seed(1)
for(k in 1:11)
{
knn.pred <- knn(train=trainset[, variables], test=testset[, variables], cl=trainset$final_quality, k=k)
test_error$error[k]= round(sum(knn.pred!=testset$final_quality)/nrow(testset)*100,2)
}
print(test_error)
set.seed(1)
rows <- sample(x=nrow(fulldataset), size=0.8*nrow(fulldataset))
trainset <- fulldataset[rows, ]
testset <- fulldataset[-rows, ]
library (MASS)
lda.fit <- lda(final_quality ~ fixed.acidity+volatile.acidity+citric.acid+residual.sugar+chlorides+free.sulfur.dioxide+total.sulfur.dioxide+density+pH+sulphates+alcohol, data=trainset)
lda.pred <- predict(lda.fit, testset)
confusion_matrix_lda <- table(testset$final_quality, lda.pred$class)
print(confusion_matrix_lda)
test_error_lda <- sum(lda.pred$class!=testset$final_quality)/nrow(testset)
sprintf("The test error is: %f", test_error_lda)
qda.fit <- qda(final_quality ~ fixed.acidity+volatile.acidity+citric.acid+residual.sugar+chlorides+free.sulfur.dioxide+total.sulfur.dioxide+density+pH+sulphates+alcohol, data=trainset)
qda.pred <- predict(qda.fit, testset)
confusion_matrix_qda <- table(testset$final_quality, qda.pred$class)
print(confusion_matrix_qda)
test_error_qda <- sum(qda.pred$class!=testset$final_quality)/nrow(testset)
sprintf("The test error is: %f", test_error_qda)
## a.
# Numerical summary of data
str(MyData)
library(psych)
describe(MyData)
summary(MyData)
# Graphical summary of data
pairs(MyData, main="Scatter Plot of Variables")
# Correlation of attributes
z <- cor(MyData)
z[lower.tri(z,diag=TRUE)]=NA
z=as.data.frame(as.table(z))
z=na.omit(z)
z=z[order(-abs(z$Freq)),]
head(z, n=10)
library(class)
# Read CSV from working directory into R
MyData <- read.csv(file="redwine.csv", header=TRUE, sep=",")
### Question 2
## a.
# Numerical summary of data
str(MyData)
library(psych)
describe(MyData)
summary(MyData)
# Graphical summary of data
pairs(MyData, main="Scatter Plot of Variables")
# Correlation of attributes
z <- cor(MyData)
z[lower.tri(z,diag=TRUE)]=NA
z=as.data.frame(as.table(z))
z=na.omit(z)
z=z[order(-abs(z$Freq)),]
head(z, n=10)
library(class)
# Read CSV from working directory into R
MyData <- read.csv(file="redwine.csv", header=TRUE, sep=",")
### Question 2
## a.
# Numerical summary of data
str(MyData)
library(psych)
describe(MyData)
summary(MyData)
# Graphical summary of data
pairs(MyData, main="Scatter Plot of Variables")
# Correlation of attributes
z <- cor(MyData)
z[lower.tri(z,diag=TRUE)]=NA
z=as.data.frame(as.table(z))
z=na.omit(z)
z=z[order(-abs(z$Freq)),]
head(z, n=10)
# After exploring the dataset, I generated a correlation matrix to understand
# how the attributes are related to each other. I created a list of these
# relationships, sorted them, and printed out the ones with the largest absolute
# values. Obviously, a correlation of 1 would be the highest signify a perfect,
# positive correlation whereas a -1 would signify a perfect, negative
# correlation.
# The highest correlation patterns in the data seem to be between fixed.acidity
# & pH, fixed.acidity % citric.acid, fixed.acidity & density, and
# freesulfur.dioxide & total.sulfur.dioxide. All of them have roughly a 0.68
# correlation - only fixed.acidity and pH have an inverse relationship. Although
# these correlations are not extremely high, they indicate that there is some
# dependency of variables on each other which is potentially harmful to us when
# building a model.
## b.
# Create binary variable final_quality using mean
MyData$final_quality <- with(ifelse(quality>mean(quality), 1, 0), data=MyData)
# Creating dataset without original quality attribute
myvars <- names(MyData) %in% c("quality")
fulldataset <- MyData[!myvars]
head(fulldataset, n=1)
# Splitting data into train and test
set.seed(1)
rows <- sample(x=nrow(fulldataset), size=.80*nrow(fulldataset))
trainset <- fulldataset[rows, ]
testset <- fulldataset[-rows, ]
# Logistic Regression
glm.fit <- glm(final_quality ~ fixed.acidity+volatile.acidity+citric.acid+residual.sugar+chlorides+free.sulfur.dioxide+total.sulfur.dioxide+density+pH+sulphates+alcohol, data=trainset, family=binomial)
summary(glm.fit)
glm.probs <- predict(glm.fit, testset, type="response")
glm.preds <- ifelse(glm.probs>0.5, 1, 0)
confusion_matrix_glm <- table(testset$final_quality,glm.preds)
print(confusion_matrix_glm)
# Fraction of Correct Predictions
Correct_Predictions_fraction = (confusion_matrix_glm[1,1]+confusion_matrix_glm[2,2])/sum(confusion_matrix_glm)
sprintf("Overall Fraction of Correct Predictions are: %f",Correct_Predictions_fraction)
es","free.sulfur.dioxide","total.sulfur.dioxide","density","pH","sulphates","alcohol"))
test_error <- data.frame("k"=1:11)
set.seed(1)
for(k in 1:11)
{
knn.pred <- knn(train=trainset[, variables], test=testset[, variables], cl=trainset$final_quality, k=k)
test_error$error[k]= round(sum(knn.pred!=testset$final_quality)/nrow(testset)*100,2)
}
print(test_error)
variables <- which(names(fulldataset)%in%c("fixed.acidity","volatile.acidity","citric.acid","residual.sugar","chlorides","free.sulfur.dioxide","total.sulfur.dioxide","density","pH","sulphates","alcohol"))
test_error <- data.frame("k"=1:11)
set.seed(1)
for(k in 1:11)
{
knn.pred <- knn(train=trainset[, variables], test=testset[, variables], cl=trainset$final_quality, k=k)
test_error$error[k]= round(sum(knn.pred!=testset$final_quality)/nrow(testset)*100,2)
}
print(test_error)
set.seed(1)
for(k in 1:11)
{
knn.pred <- knn(train=trainset[, variables], test=testset[, variables], cl=trainset$final_quality, k=k)
test_error$error[k]= round(sum(knn.pred=testset$final_quality)/nrow(testset)*100,2)
}
print(test_error)
set.seed(1)
for(k in 1:11)
{
knn.pred <- knn(train=trainset[, variables], test=testset[, variables], cl=trainset$final_quality, k=k)
test_error$error[k]= round(sum(knn.pred!=testset$final_quality)/nrow(testset)*100,2)
}
print(test_error)
set.seed(1)
rows <- sample(x=nrow(fulldataset), size=0.8*nrow(fulldataset))
trainset <- fulldataset[rows, ]
testset <- fulldataset[-rows, ]
library (MASS)
lda.fit <- lda(final_quality ~ fixed.acidity+volatile.acidity+citric.acid+residual.sugar+chlorides+free.sulfur.dioxide+total.sulfur.dioxide+density+pH+sulphates+alcohol, data=trainset)
lda.pred <- predict(lda.fit, testset)
confusion_matrix_lda <- table(testset$final_quality, lda.pred$class)
print(confusion_matrix_lda)
test_error_lda <- sum(lda.pred$class!=testset$final_quality)/nrow(testset)
sprintf("The test error is: %f", test_error_lda)
print(confusion_matrix_glm)
qda.fit <- qda(final_quality ~ fixed.acidity+volatile.acidity+citric.acid+residual.sugar+chlorides+free.sulfur.dioxide+total.sulfur.dioxide+density+pH+sulphates+alcohol, data=trainset)
qda.pred <- predict(qda.fit, testset)
confusion_matrix_qda <- table(testset$final_quality, qda.pred$class)
print(confusion_matrix_qda)
test_error_qda <- sum(qda.pred$class!=testset$final_quality)/nrow(testset)
sprintf("The test error for QDA is: %f", test_error_qda)
library(class)
# Read CSV from working directory into R
MyData <- read.csv(file="redwine.csv", header=TRUE, sep=",")
### Question 2
## a.
# Numerical summary of data
str(MyData)
library(psych)
describe(MyData)
summary(MyData)
# Graphical summary of data
pairs(MyData, main="Scatter Plot of Variables")
# Correlation of attributes
z <- cor(MyData)
z[lower.tri(z,diag=TRUE)]=NA
z=as.data.frame(as.table(z))
z=na.omit(z)
z=z[order(-abs(z$Freq)),]
head(z, n=10)
# After exploring the dataset, I generated a correlation matrix to understand
# how the attributes are related to each other. I created a list of these
# relationships, sorted them, and printed out the ones with the largest absolute
# values. Obviously, a correlation of 1 would be the highest signify a perfect,
# positive correlation whereas a -1 would signify a perfect, negative
# correlation.
# The highest correlation patterns in the data seem to be between fixed.acidity
# & pH, fixed.acidity % citric.acid, fixed.acidity & density, and
# freesulfur.dioxide & total.sulfur.dioxide. All of them have roughly a 0.68
# correlation - only fixed.acidity and pH have an inverse relationship. Although
# these correlations are not extremely high, they indicate that there is some
# dependency of variables on each other which is potentially harmful to us when
# building a model.
## b.
# Create binary variable final_quality using mean
MyData$final_quality <- with(ifelse(quality>mean(quality), 1, 0), data=MyData)
# Creating dataset without original quality attribute
myvars <- names(MyData) %in% c("quality")
fulldataset <- MyData[!myvars]
head(fulldataset, n=1)
# Splitting data into train and test
set.seed(1)
rows <- sample(x=nrow(fulldataset), size=.80*nrow(fulldataset))
trainset <- fulldataset[rows, ]
testset <- fulldataset[-rows, ]
# Logistic Regression
glm.fit <- glm(final_quality ~ fixed.acidity+volatile.acidity+citric.acid+residual.sugar+chlorides+free.sulfur.dioxide+total.sulfur.dioxide+density+pH+sulphates+alcohol, data=trainset, family=binomial)
summary(glm.fit)
# Atleast 4 of the predictors appear to be statistically significant. alcohol,
# sulphates, total.sulfur.dioxide, and volative.acidity all have very small p
# values meaning they have the largest impact on the response variable of final
# quality.
## c.
# Confusion Matrix
glm.probs <- predict(glm.fit, testset, type="response")
glm.preds <- ifelse(glm.probs>0.5, 1, 0)
confusion_matrix_glm <- table(testset$final_quality,glm.preds)
print(confusion_matrix_glm)
# Fraction of Correct Predictions
Correct_Predictions_fraction = (confusion_matrix_glm[1,1]+confusion_matrix_glm[2,2])/sum(confusion_matrix_glm)
sprintf("Overall Fraction of Correct Predictions are: %f",Correct_Predictions_fraction)
# The confusion tells us about the performance of the model. There were True
# Negatives (105) and True Positives (143) and, as can be seen above, constitute
# about 77.5% of results. The rest were misclassified - so about 22.5%. 27
# points were False Positives and 45 were False Negatives. This tells us our
# logistic regression model is wrong about one fourth of the time and tends to
# be too conservative. It is wrongly classifying good wine (in class 1 that have
# quality above the mean) as bad wine (in class 0 with quality below mean) more
# than it is classifying bad wine as good wine (although that is happening a
# fair bit as well).
## d.
variables <- which(names(fulldataset)%in%c("fixed.acidity","volatile.acidity","citric.acid","residual.sugar","chlorides","free.sulfur.dioxide","total.sulfur.dioxide","density","pH","sulphates","alcohol"))
test_error <- data.frame("k"=1:11)
set.seed(1)
for(k in 1:11)
{
knn.pred <- knn(train=trainset[, variables], test=testset[, variables], cl=trainset$final_quality, k=k)
test_error$error[k]= round(sum(knn.pred!=testset$final_quality)/nrow(testset)*100,2)
}
print(test_error)
# As seen above, the model with the lowest test error is when k=1 with an error
# of approximately 25 and therefore can be concluded to be performing the best
# on this dataset.
### Question 3
## a.
# Split data into training and test - 80/20
set.seed(1)
rows <- sample(x=nrow(fulldataset), size=0.8*nrow(fulldataset))
trainset <- fulldataset[rows, ]
testset <- fulldataset[-rows, ]
## b.
# LDA
library (MASS)
lda.fit <- lda(final_quality ~ fixed.acidity+volatile.acidity+citric.acid+residual.sugar+chlorides+free.sulfur.dioxide+total.sulfur.dioxide+density+pH+sulphates+alcohol, data=trainset)
lda.pred <- predict(lda.fit, testset)
confusion_matrix_lda <- table(testset$final_quality, lda.pred$class)
print(confusion_matrix_lda)
test_error_lda <- sum(lda.pred$class!=testset$final_quality)/nrow(testset)
sprintf("The test error for LDA is: %f", test_error_lda)
## c.
# QDA
qda.fit <- qda(final_quality ~ fixed.acidity+volatile.acidity+citric.acid+residual.sugar+chlorides+free.sulfur.dioxide+total.sulfur.dioxide+density+pH+sulphates+alcohol, data=trainset)
qda.pred <- predict(qda.fit, testset)
confusion_matrix_qda <- table(testset$final_quality, qda.pred$class)
print(confusion_matrix_qda)
test_error_qda <- sum(qda.pred$class!=testset$final_quality)/nrow(testset)
sprintf("The test error for QDA is: %f", test_error_qda)
set.seed(1)
x <- rnorm(100)
epsilon <- rnorm(100)
b0 <- 6
b1 <- 3
b2 <- 9
b3 <- -3
y <- b0 + b1 * x + b2 * x^2 + b3 * x^3 + epsilon
library(leaps)
data.full <- data.frame(y = y, x = x)
regfwd <- regsubsets(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6), data = data.full, nvmax = 10, method = "forward")
regfwd.summary <- summary(regfwd)
coef(regfit.fwd, which.max(regfwd.summary$adjr2))
par(mfrow = c(2, 2))
plot(regfwd.summary$cp, xlab = "Number of variables", ylab = "C_p", type = "l")
points(which.min(regfwd.summary$cp), regfwd.summary$cp[which.min(regfwd.summary$cp)], col = "red", cex = 2, pch = 20)
plot(regfwd.summary$bic, xlab = "Number of variables", ylab = "BIC", type = "l")
points(which.min(regfwd.summary$bic), regfwd.summary$bic[which.min(regfwd.summary$bic)], col = "red", cex = 2, pch = 20)
plot(regfwd.summary$adjr2, xlab = "Number of variables", ylab = "Adjusted R^2", type = "l")
points(which.max(regfwd.summary$adjr2), regfwd.summary$adjr2[which.max(regfwd.summary$adjr2)], col = "red", cex = 2, pch = 20)
mtext("Plots of C_p, BIC and adjusted R^2 for forward stepwise selection", side = 3, line = -2, outer = TRUE)
regbwd <- regsubsets(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6), data = data.full, nvmax = 10, method = "backward")
regbwd.summary <- summary(regbwd)
coef(regfit.bwd, which.max(regbwd.summary$adjr2))
par(mfrow = c(2, 2))
plot(regbwd.summary$cp, xlab = "Number of variables", ylab = "C_p", type = "l")
points(which.min(regbwd.summary$cp), regbwd.summary$cp[which.min(regbwd.summary$cp)], col = "red", cex = 2, pch = 20)
plot(regbwd.summary$bic, xlab = "Number of variables", ylab = "BIC", type = "l")
points(which.min(regbwd.summary$bic), regbwd.summary$bic[which.min(regbwd.summary$bic)], col = "red", cex = 2, pch = 20)
plot(regbwd.summary$adjr2, xlab = "Number of variables", ylab = "Adjusted R^2", type = "l")
points(which.max(regbwd.summary$adjr2), regbwd.summary$adjr2[which.max(regbwd.summary$adjr2)], col = "red", cex = 2, pch = 20)
mtext("Plots of C_p, BIC and adjusted R^2 for backward stepwise selection", side = 3, line = -2, outer = TRUE)
library(glmnet)
xmatrix <- model.matrix(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6), data = data.full)[, -1]
cv.lasso <- cv.glmnet(xmatrix, y, alpha = 1)
plot(cv.lasso)
bestlam <- cv.lasso$lambda.min
bestlam
fit.lasso <- glmnet(xmat, y, alpha = 1)
predict(fit.lasso, s = bestlam, type = "coefficients")[1:7, ]
set.seed(1)
x <- rnorm(100)
epsilon <- rnorm(100)
b0 <- 6
b1 <- 3
b2 <- 9
b3 <- -3
y <- b0 + b1 * x + b2 * x^2 + b3 * x^3 + epsilon
library(leaps)
data.full <- data.frame(y = y, x = x)
regfwd <- regsubsets(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6), data = data.full, nvmax = 10, method = "forward")
regfwd.summary <- summary(regfwd)
coef(regfit.fwd, which.max(regfwd.summary$adjr2))
par(mfrow = c(2, 2))
plot(regfwd.summary$cp, xlab = "Number of variables", ylab = "C_p", type = "l")
points(which.min(regfwd.summary$cp), regfwd.summary$cp[which.min(regfwd.summary$cp)], col = "red", cex = 2, pch = 20)
plot(regfwd.summary$bic, xlab = "Number of variables", ylab = "BIC", type = "l")
points(which.min(regfwd.summary$bic), regfwd.summary$bic[which.min(regfwd.summary$bic)], col = "red", cex = 2, pch = 20)
plot(regfwd.summary$adjr2, xlab = "Number of variables", ylab = "Adjusted R^2", type = "l")
points(which.max(regfwd.summary$adjr2), regfwd.summary$adjr2[which.max(regfwd.summary$adjr2)], col = "red", cex = 2, pch = 20)
mtext("Plots of C_p, BIC and adjusted R^2 for forward stepwise selection", side = 3, line = -2, outer = TRUE)
regbwd <- regsubsets(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6), data = data.full, nvmax = 10, method = "backward")
regbwd.summary <- summary(regbwd)
coef(regfit.bwd, which.max(regbwd.summary$adjr2))
par(mfrow = c(2, 2))
plot(regbwd.summary$cp, xlab = "Number of variables", ylab = "C_p", type = "l")
points(which.min(regbwd.summary$cp), regbwd.summary$cp[which.min(regbwd.summary$cp)], col = "red", cex = 2, pch = 20)
plot(regbwd.summary$bic, xlab = "Number of variables", ylab = "BIC", type = "l")
points(which.min(regbwd.summary$bic), regbwd.summary$bic[which.min(regbwd.summary$bic)], col = "red", cex = 2, pch = 20)
plot(regbwd.summary$adjr2, xlab = "Number of variables", ylab = "Adjusted R^2", type = "l")
points(which.max(regbwd.summary$adjr2), regbwd.summary$adjr2[which.max(regbwd.summary$adjr2)], col = "red", cex = 2, pch = 20)
mtext("Plots of C_p, BIC and adjusted R^2 for backward stepwise selection", side = 3, line = -2, outer = TRUE)
library(glmnet)
xmatrix <- model.matrix(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6), data = data.full)[, -1]
cv.lasso <- cv.glmnet(xmatrix, y, alpha = 1)
plot(cv.lasso)
bestlam <- cv.lasso$lambda.min
bestlam
fit.lasso <- glmnet(xmat, y, alpha = 1)
predict(fit.lasso, s = bestlam, type = "coefficients")[1:7, ]
fit.lasso <- glmnet(xmatrix, y, alpha = 1)
predict(fit.lasso, s = bestlam, type = "coefficients")[1:7, ]
